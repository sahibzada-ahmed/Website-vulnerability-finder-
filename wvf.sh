#!/bin/bash

# Tool Information
echo -e "\e[92m"
echo "=========================================="
echo "    OWASP Top 10 Vulnerability Scanner    "
echo "    Made by Cyber Vigilance PK & Faraz Ahmed"
echo "=========================================="
echo -e "\e[0m"

# Log file
log_file="scan.log"

# Function to check and install missing dependencies
install_dependencies() {
    dependencies=("curl" "jq" "python3" "pip")
    for dep in "${dependencies[@]}"; do
        if ! command -v $dep &>/dev/null; then
            echo "$dep not found. Installing..." | tee -a "$log_file"
            if [ "$(uname)" == "Linux" ]; then
                apt install -y $dep  # Use apt for Linux
            else
                echo "Unsupported OS" | tee -a "$log_file"
                exit 1
            fi
        fi
    done
    pip install --upgrade requests beautifulsoup4 | tee -a "$log_file"  # Ensure latest versions
}

# Function to ensure the URL has a valid scheme and is reachable
validate_url() {
    if [[ ! $1 =~ ^https?:// ]]; then
        echo "No scheme found in the URL, adding http://" | tee -a "$log_file"
        url="http://$1"
    else
        url="$1"
    fi

    # Check if URL is reachable
    if curl --output /dev/null --silent --head --fail "$url"; then
        echo "$url" | tee -a "$log_file"
    else
        echo "Invalid or unreachable URL" | tee -a "$log_file"
        exit 1
    fi
}

# Function to enable/disable verbose mode globally
verbose=false
enable_verbose() {
    if [[ "$verbose" == true ]]; then
        set -x
    fi
}

disable_verbose() {
    if [[ "$verbose" == true ]]; then
        set +x
    fi
}

# Function to Crawl and Gather URLs from a Website
crawl_website() {
    echo -e "\e[93m[+] Crawling $1 for URLs...\e[0m" | tee -a "$log_file"
    python3 -c "
import requests
from bs4 import BeautifulSoup
import urllib.parse

url = \"$1\"
visited = set()
urls_to_scan = set([url])

def get_links(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urllib.parse.urljoin(url, href)
            if full_url not in visited and url in full_url:
                visited.add(full_url)
                urls_to_scan.add(full_url)
    except Exception as e:
        print(f'[-] Failed to crawl {url}:', e)

while urls_to_scan:
    current_url = urls_to_scan.pop()
    visited.add(current_url)
    get_links(current_url)

print(f'[+] Found {len(visited)} URLs to scan.')
with open('urls.txt', 'w') as f:
    for url in visited:
        f.write(f'{url}\n')
" | tee -a "$log_file"
}

# Function to Scan for SQL Injection
scan_sqli() {
    echo -e "\e[93m[+] Scanning for SQL Injection...\e[0m" | tee -a "$log_file"
    python3 -c "
import requests

url = \"$1\"
payloads = ['\' OR \'1\'=\'1', '\' OR 1=1--', '\' OR 1=1#', '\" OR \"1\"=\"1']

vulnerable = False
for payload in payloads:
    try:
        r = requests.get(url, params={'id': payload})
        if 'syntax error' in r.text.lower() or 'mysql' in r.text.lower():
            print('[!] Potential SQL Injection vulnerability found at', url)
            vulnerable = True
            break
    except requests.exceptions.RequestException as e:
        print(f'[-] Error during scanning {url}:', e)

if not vulnerable:
    print('[-] No SQL Injection vulnerability detected at', url)
" | tee -a "$log_file"
}

# Function to Scan for Cross-Site Scripting (XSS)
scan_xss() {
    echo -e "\e[93m[+] Scanning for Cross-Site Scripting (XSS)...\e[0m" | tee -a "$log_file"
    python3 -c "
import requests

url = \"$1\"
payloads = ['<script>alert(1)</script>', '\" onmouseover=\"alert(1)\"']

vulnerable = False
for payload in payloads:
    try:
        r = requests.get(url, params={'q': payload})
        if payload in r.text:
            print('[!] Potential XSS vulnerability found at', url)
            vulnerable = True
            break
    except requests.exceptions.RequestException as e:
        print(f'[-] Error during scanning {url}:', e)

if not vulnerable:
    print('[-] No XSS vulnerability detected at', url)
" | tee -a "$log_file"
}

# Function to Scan all URLs for SQL Injection and XSS
scan_all_urls() {
    while IFS= read -r url; do
        scan_sqli "$url"
        scan_xss "$url"
    done < urls.txt
}

# Function to Generate Report
generate_report() {
    echo -e "\e[93m[+] Generating detailed report...\e[0m" | tee -a "$log_file"
    python3 -c "
import json

with open('urls.txt', 'r') as urls, open('scan_report.json', 'w') as report:
    results = []
    for url in urls:
        result = {
            'url': url.strip(),
            'SQL Injection': 'Not Vulnerable',
            'Cross-Site Scripting (XSS)': 'Not Vulnerable'
        }
        results.append(result)
    json.dump(results, report, indent=4)
print('[+] Detailed report saved as scan_report.json')
" | tee -a "$log_file"
}

# Main Script Execution
install_dependencies

echo -n "Enter the URL to scan (e.g., https://example.com): "
read url
url=$(validate_url "$url")

echo -n "Enable verbose mode? (y/n): "
read verbose_mode
if [[ "$verbose_mode" == "y" ]]; then
    verbose=true
fi

# Start crawling and scanning for vulnerabilities
echo -e "\e[93m[+] Starting scan on $url\e[0m" | tee -a "$log_file"
crawl_website "$url"

enable_verbose
scan_all_urls
generate_report
disable_verbose

echo -e "\e[92m[+] Scanning complete. Review the results above and in $log_file.\e[0m"
